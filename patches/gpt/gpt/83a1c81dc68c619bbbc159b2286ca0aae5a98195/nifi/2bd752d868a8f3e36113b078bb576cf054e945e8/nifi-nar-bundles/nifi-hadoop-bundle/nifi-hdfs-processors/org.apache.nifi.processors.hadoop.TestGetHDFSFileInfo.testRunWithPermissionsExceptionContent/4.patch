Test File Path: projects/2bd752d868a8f3e36113b078bb576cf054e945e8/nifi/nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/test/java/org/apache/nifi/processors/hadoop/TestGetHDFSFileInfo.java

    Original Test Method:
     public void testRunWithPermissionsExceptionContent() throws Exception {

        setFileSystemBasicTree(proc.fileSystem);
        proc.fileSystem.addFileStatus(proc.fileSystem.newDir("/some/home/mydir/dir1"), proc.fileSystem.newDir("/some/home/mydir/dir1/list_exception_java.io.InterruptedIOException"));

        runner.setIncomingConnection(false);
        runner.setProperty(GetHDFSFileInfo.FULL_PATH, "/some/home/mydir");
        runner.setProperty(GetHDFSFileInfo.RECURSE_SUBDIRS, "true");
        runner.setProperty(GetHDFSFileInfo.IGNORE_DOTTED_DIRS, "true");
        runner.setProperty(GetHDFSFileInfo.IGNORE_DOTTED_FILES, "true");
        runner.setProperty(GetHDFSFileInfo.DESTINATION, GetHDFSFileInfo.DESTINATION_CONTENT);
        runner.setProperty(GetHDFSFileInfo.GROUPING, GetHDFSFileInfo.GROUP_ALL);

        runner.run();

        runner.assertTransferCount(GetHDFSFileInfo.REL_ORIGINAL, 0);
        runner.assertTransferCount(GetHDFSFileInfo.REL_SUCCESS, 1);
        runner.assertTransferCount(GetHDFSFileInfo.REL_FAILURE, 0);
        runner.assertTransferCount(GetHDFSFileInfo.REL_NOT_FOUND, 0);

        final MockFlowFile mff = runner.getFlowFilesForRelationship(GetHDFSFileInfo.REL_SUCCESS).get(0);
        mff.assertContentEquals(Paths.get("src/test/resources/TestGetHDFSFileInfo/testRunWithPermissionsExceptionContent.json"));
    }

    
test_code:
public void testRunWithPermissionsExceptionContent() throws Exception {

    setFileSystemBasicTree(proc.fileSystem);
    proc.fileSystem.addFileStatus(proc.fileSystem.newDir("/some/home/mydir/dir1"), proc.fileSystem.newDir("/some/home/mydir/dir1/list_exception_java.io.InterruptedIOException"));

    runner.setIncomingConnection(false);
    runner.setProperty(GetHDFSFileInfo.FULL_PATH, "/some/home/mydir");
    runner.setProperty(GetHDFSFileInfo.RECURSE_SUBDIRS, "true");
    runner.setProperty(GetHDFSFileInfo.IGNORE_DOTTED_DIRS, "true");
    runner.setProperty(GetHDFSFileInfo.IGNORE_DOTTED_FILES, "true");
    runner.setProperty(GetHDFSFileInfo.DESTINATION, GetHDFSFileInfo.DESTINATION_CONTENT);
    runner.setProperty(GetHDFSFileInfo.GROUPING, GetHDFSFileInfo.GROUP_ALL);

    runner.run();

    runner.assertTransferCount(GetHDFSFileInfo.REL_ORIGINAL, 0);
    runner.assertTransferCount(GetHDFSFileInfo.REL_SUCCESS, 1);
    runner.assertTransferCount(GetHDFSFileInfo.REL_FAILURE, 0);
    runner.assertTransferCount(GetHDFSFileInfo.REL_NOT_FOUND, 0);

    final MockFlowFile mff = runner.getFlowFilesForRelationship(GetHDFSFileInfo.REL_SUCCESS).get(0);
    String actualContent = new String(mff.toByteArray());
    String expectedContent = new String(Files.readAllBytes(Paths.get("src/test/resources/TestGetHDFSFileInfo/testRunWithPermissionsExceptionContent.json")));

    ObjectMapper mapper = new ObjectMapper();
    mapper.configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);
    JsonNode actualJson = mapper.readTree(actualContent);
    JsonNode expectedJson = mapper.readTree(expectedContent);

    Set<JsonNode> actualSet = new HashSet<>(actualJson.size());
    if (actualJson.isArray()) {
        actualJson.forEach(actualSet::add);
    }

    Set<JsonNode> expectedSet = new HashSet<>(expectedJson.size());
    if (expectedJson.isArray()) {
        expectedJson.forEach(expectedSet::add);
    }

    assertEquals(expectedSet, actualSet);
}

import:
['import com.fasterxml.jackson.databind.node.ArrayNode;', 'import com.fasterxml.jackson.databind.node.ObjectNode;']
pom:

<!-- No changes needed -->
